{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "from sklearn.ensemble import IsolationForest\n",
    "#from keras.layers import Dense\n",
    "#from keras.models import Sequential\n",
    "#from keras.optimizers import Adam\n",
    "\n",
    "class TFModel:\n",
    "    def _create_layer(self, input_layer, layer_weights, layer_bias, activation_func):\n",
    "        layer = tf.add(tf.matmul(input_layer, layer_weights), layer_bias)\n",
    "        return activation_func(layer)\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layer_sizes):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        layer_sizes = [self.num_inputs] + hidden_layer_sizes + [self.num_outputs]\n",
    "        num_layers = len(layer_sizes)\n",
    "        \n",
    "        weights = list(\n",
    "            map(\n",
    "                lambda input_size, output_size: tf.Variable(\n",
    "                    tf.random_normal(shape=[input_size, output_size])\n",
    "                ),\n",
    "                layer_sizes,\n",
    "                layer_sizes[1:],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        biases = list(\n",
    "            map(\n",
    "                lambda layer_size: tf.Variable(tf.random_normal(shape=[layer_size])),\n",
    "                layer_sizes[1:],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        activation_funcs = list(\n",
    "            map(\n",
    "                lambda layer_index: tf.identity\n",
    "                if layer_index == num_layers - 2\n",
    "                else tf.nn.relu,\n",
    "                range(num_layers - 1),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self._input = tf.placeholder(shape=[None, self.num_inputs], dtype=tf.float32)\n",
    "        layer = self._input\n",
    "        for layer_weights, layer_bias, activation_func in zip(\n",
    "            weights, biases, activation_funcs\n",
    "        ):\n",
    "            layer = self._create_layer(\n",
    "                layer, layer_weights, layer_bias, activation_func\n",
    "            )\n",
    "        \n",
    "        self._output = layer\n",
    "        \n",
    "        self._target_output = tf.placeholder(\n",
    "            shape=[None, self.num_outputs], dtype=tf.float32\n",
    "        )\n",
    "        loss = tf.losses.mean_squared_error(self._target_output, self._output)\n",
    "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()\n",
    "    \n",
    "    def predict(self, sess, data_input):\n",
    "        return sess.run(self._output, feed_dict={self._input: data_input})\n",
    "\n",
    "    def predict_batch(self, sess, inputs):\n",
    "        return sess.run(self._output, feed_dict={self._input: inputs})\n",
    "\n",
    "    def train_batch(self, sess, inputs, outputs):\n",
    "        sess.run(\n",
    "            self._optimizer,\n",
    "            feed_dict={self._input: inputs, self._target_output: outputs},\n",
    "        )\n",
    "        \n",
    "def compile_keras_model(data_input, output, number_of_nodes_in_hidden_layer=3, number_of_hidden_layers=2, hidden_layer_activation='relu', output_layer_activation='sigmoid', loss='mean_squared_error'):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(number_of_nodes_in_hidden_layer, activation=hidden_layer_activation, input_shape=(len(data_input.columns),)))\n",
    "    for _ in range(number_of_hidden_layers - 1):  \n",
    "        model.add(keras.layers.Dense(number_of_nodes_in_hidden_layer, activation=hidden_layer_activation))\n",
    "    model.add(keras.layers.Dense(1, activation=output_layer_activation))\n",
    "    \n",
    "    adam = keras.optimizers.Adam()\n",
    "    model.compile(loss=loss, optimizer=adam, metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outliers using Isolation Forest\n",
    "def find_outliers_isolation_forest(dataframe):\n",
    "    isolation_forest = IsolationForest(contamination='auto', behaviour='new')\n",
    "    isolation_forest.fit(dataframe)\n",
    "    return isolation_forest.predict(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cceb6ccbf8d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# find outliers using Isolation Forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moutliers_isolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_outliers_isolation_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"outliers_isolation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutliers_isolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-840ccf2bf2f1>\u001b[0m in \u001b[0;36mfind_outliers_isolation_forest\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_outliers_isolation_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0misolation_forest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbehaviour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'new'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0misolation_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0misolation_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\iforest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m                  FutureWarning)\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[1;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/yes_date_set.csv\")\n",
    "data = data.drop('date', axis=1)\n",
    "\n",
    "# find outliers using Isolation Forest\n",
    "outliers_isolation = find_outliers_isolation_forest(data)\n",
    "data.insert(0, \"outliers_isolation\", outliers_isolation, True)\n",
    "\n",
    "# drop outliers based on Isolation Forest method\n",
    "data = data.drop(data[data.outliers_isolation == -1.0].index)\n",
    "data = data.drop('outliers_isolation', axis=1)\n",
    "\n",
    "X = data.drop(['trip_count', 'Holiday_None'], axis=1)\n",
    "Y = data['trip_count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# normalize input\n",
    "standard_scaler = StandardScaler()\n",
    "X_train = standard_scaler.fit_transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "print(\"training X\")\n",
    "print(X_train)\n",
    "print(\"training Y\")\n",
    "print(y_train)\n",
    "print(\"test X\")\n",
    "print(X_test)\n",
    "print(\"test Y\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3846 samples, validate on 1649 samples\n",
      "Epoch 1/150\n",
      "3846/3846 [==============================] - 1s 234us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 2/150\n",
      "3846/3846 [==============================] - 0s 91us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 3/150\n",
      "3846/3846 [==============================] - 0s 102us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 4/150\n",
      "3846/3846 [==============================] - 0s 118us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 5/150\n",
      "3846/3846 [==============================] - 0s 110us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 6/150\n",
      "3846/3846 [==============================] - 0s 101us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 7/150\n",
      "3846/3846 [==============================] - 0s 94us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 8/150\n",
      "3846/3846 [==============================] - 0s 123us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 9/150\n",
      "3846/3846 [==============================] - 1s 133us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 10/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 11/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 12/150\n",
      "3846/3846 [==============================] - ETA: 0s - loss: nan - acc: 0.04 - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 13/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 14/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 15/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 16/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 17/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 18/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 19/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 20/150\n",
      "3846/3846 [==============================] - 0s 79us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 21/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 22/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 23/150\n",
      "3846/3846 [==============================] - 0s 83us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 24/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 25/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 26/150\n",
      "3846/3846 [==============================] - 0s 83us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 27/150\n",
      "3846/3846 [==============================] - 0s 84us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 28/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 29/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 30/150\n",
      "3846/3846 [==============================] - 0s 110us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 31/150\n",
      "3846/3846 [==============================] - 0s 112us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 32/150\n",
      "3846/3846 [==============================] - 0s 99us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 33/150\n",
      "3846/3846 [==============================] - 0s 99us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 34/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 35/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 36/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 37/150\n",
      "3846/3846 [==============================] - 0s 83us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 38/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 39/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 40/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 41/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 42/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 43/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 44/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 45/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 46/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 47/150\n",
      "3846/3846 [==============================] - 0s 91us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 48/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 49/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 50/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 51/150\n",
      "3846/3846 [==============================] - 0s 84us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 52/150\n",
      "3846/3846 [==============================] - 0s 92us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 53/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 54/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 55/150\n",
      "3846/3846 [==============================] - 0s 94us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 56/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 57/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 58/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 59/150\n",
      "3846/3846 [==============================] - 0s 91us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 60/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 61/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 63/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 64/150\n",
      "3846/3846 [==============================] - 0s 96us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 65/150\n",
      "3846/3846 [==============================] - 0s 90us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 66/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 67/150\n",
      "3846/3846 [==============================] - 0s 90us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 68/150\n",
      "3846/3846 [==============================] - 0s 84us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 69/150\n",
      "3846/3846 [==============================] - 0s 92us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 70/150\n",
      "3846/3846 [==============================] - 0s 90us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 71/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 72/150\n",
      "3846/3846 [==============================] - 0s 90us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 73/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 74/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 75/150\n",
      "3846/3846 [==============================] - 0s 86us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 76/150\n",
      "3846/3846 [==============================] - 0s 100us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 77/150\n",
      "3846/3846 [==============================] - 0s 117us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 78/150\n",
      "3846/3846 [==============================] - 0s 113us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 79/150\n",
      "3846/3846 [==============================] - 0s 105us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 80/150\n",
      "3846/3846 [==============================] - 0s 101us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 81/150\n",
      "3846/3846 [==============================] - 0s 113us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 82/150\n",
      "3846/3846 [==============================] - 0s 97us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 83/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 84/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 85/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 86/150\n",
      "3846/3846 [==============================] - 0s 94us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 87/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 88/150\n",
      "3846/3846 [==============================] - 0s 101us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 89/150\n",
      "3846/3846 [==============================] - 0s 84us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 90/150\n",
      "3846/3846 [==============================] - 0s 95us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 91/150\n",
      "3846/3846 [==============================] - 0s 87us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 92/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 93/150\n",
      "3846/3846 [==============================] - 0s 89us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 94/150\n",
      "3846/3846 [==============================] - 0s 110us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 95/150\n",
      "3846/3846 [==============================] - 0s 83us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 96/150\n",
      "3846/3846 [==============================] - 0s 103us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 97/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 98/150\n",
      "3846/3846 [==============================] - 0s 92us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 99/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 100/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 101/150\n",
      "3846/3846 [==============================] - 0s 83us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 102/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 103/150\n",
      "3846/3846 [==============================] - 0s 88us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 104/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 105/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 106/150\n",
      "3846/3846 [==============================] - 0s 84us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 107/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 108/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 109/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 110/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 111/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 112/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 113/150\n",
      "3846/3846 [==============================] - 0s 79us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 114/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 115/150\n",
      "3846/3846 [==============================] - 0s 82us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 116/150\n",
      "3846/3846 [==============================] - 0s 78us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 117/150\n",
      "3846/3846 [==============================] - 0s 78us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 118/150\n",
      "3846/3846 [==============================] - 0s 92us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 119/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 120/150\n",
      "3846/3846 [==============================] - 0s 81us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 121/150\n",
      "3846/3846 [==============================] - 0s 85us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 122/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 123/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3846/3846 [==============================] - 0s 79us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 124/150\n",
      "3846/3846 [==============================] - 0s 73us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 125/150\n",
      "3846/3846 [==============================] - 0s 74us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 126/150\n",
      "3846/3846 [==============================] - 0s 71us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 127/150\n",
      "3846/3846 [==============================] - 0s 74us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 128/150\n",
      "3846/3846 [==============================] - 0s 75us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 129/150\n",
      "3846/3846 [==============================] - 0s 75us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 130/150\n",
      "3846/3846 [==============================] - 0s 76us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 131/150\n",
      "3846/3846 [==============================] - 0s 80us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 132/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 133/150\n",
      "3846/3846 [==============================] - 0s 76us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 134/150\n",
      "3846/3846 [==============================] - 0s 76us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 135/150\n",
      "3846/3846 [==============================] - 0s 76us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 136/150\n",
      "3846/3846 [==============================] - 0s 77us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 137/150\n",
      "3846/3846 [==============================] - 0s 74us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 138/150\n",
      "3846/3846 [==============================] - 0s 71us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 139/150\n",
      "3846/3846 [==============================] - 0s 75us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 140/150\n",
      "3846/3846 [==============================] - 0s 74us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 141/150\n",
      "3846/3846 [==============================] - 0s 75us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 142/150\n",
      "3846/3846 [==============================] - 0s 71us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 143/150\n",
      "3846/3846 [==============================] - 0s 73us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 144/150\n",
      "3846/3846 [==============================] - 0s 72us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 145/150\n",
      "3846/3846 [==============================] - 0s 73us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 146/150\n",
      "3846/3846 [==============================] - 0s 70us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 147/150\n",
      "3846/3846 [==============================] - 0s 73us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 148/150\n",
      "3846/3846 [==============================] - 0s 76us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 149/150\n",
      "3846/3846 [==============================] - 0s 72us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n",
      "Epoch 150/150\n",
      "3846/3846 [==============================] - 0s 75us/sample - loss: nan - acc: 0.0424 - val_loss: nan - val_acc: 0.0528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d70d654860>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compile_keras_model(X, Y)\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alish\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\alish\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (5495,) for Tensor 'Placeholder_1:0', which has shape '(?, 1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7b4c66815ec4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2de94d58debf>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, sess, inputs, outputs)\u001b[0m\n\u001b[0;32m     74\u001b[0m         sess.run(\n\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (5495,) for Tensor 'Placeholder_1:0', which has shape '(?, 1)'"
     ]
    }
   ],
   "source": [
    "model = TFModel(len(X.columns), 1, [3, 3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(model.init)\n",
    "    \n",
    "    model.train_batch(sess, X, Y)\n",
    "    predictions = model.predict_batch(sess, X)\n",
    "    print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
